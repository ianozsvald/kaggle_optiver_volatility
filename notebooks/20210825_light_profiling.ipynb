{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8b07f27-26af-4f9f-a474-4f4ed0a155a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV_HOME: True, TRAIN_TEST False, USE_ALL_STOCK_IDS False, USE_TEST_LOCAL_6_ITEMS False\n",
      "NBR_FOR_SUBSET_OF_STOCK_IDS: 4\n",
      "In [1] used 0.0000 MiB RAM in 0.10s, peaked 0.00 MiB above current, total RAM usage 159.54 MiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "from numpy.random import default_rng\n",
    "RANDOM_STATE = 2 # random state for default_rng\n",
    "rng = default_rng(RANDOM_STATE)\n",
    "\n",
    "\n",
    "import random\n",
    "#import altair as alt\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "%load_ext line_profiler\n",
    "\n",
    "\n",
    "\n",
    "# CHECKLIST for Kaggle variant\n",
    "# Use FAST_PASS True on first pass\n",
    "# USE_ALL_STOCK_IDS False to check then True\n",
    "# USE_TEST_LOCAL_6_ITEMS must be False else we override the local test data\n",
    "# TRAIN_TEST False\n",
    "# Check on Kaggle that \"internet\" is disabled\n",
    "# First run with \"USE_ALL_STOCK_IDS=False\", flip to True, Save Version, it'll take 30 mins to run\n",
    "\n",
    "# CHECKLIST for home variant\n",
    "# USE_ALL_STOCK_IDS False for fast dev, True for proper testing\n",
    "# USE_TEST_LOCAL_6_ITEMS False for fast dev, True for proper testing\n",
    "# NBR_FOR_SUBSET_OF_STOCK_IDS 4 for quick testing\n",
    "\n",
    "t1_notebook_start = datetime.datetime.utcnow()\n",
    "\n",
    "if os.environ.get('USER') == 'ian':\n",
    "    ENV_HOME = True\n",
    "    import ipython_memory_usage\n",
    "    %ipython_memory_usage_start\n",
    "    USE_ALL_STOCK_IDS = True\n",
    "    NBR_FOR_SUBSET_OF_STOCK_IDS = 4\n",
    "    TRAIN_TEST = True\n",
    "    USE_TEST_LOCAL_6_ITEMS = False # robust local testing at home\n",
    "    MEMORY_LOCATION = 'joblib_cache'\n",
    "\n",
    "if USE_ALL_STOCK_IDS:\n",
    "    NBR_FOR_SUBSET_OF_STOCK_IDS=None\n",
    "print(f'ENV_HOME: {ENV_HOME}, TRAIN_TEST {TRAIN_TEST}, USE_ALL_STOCK_IDS {USE_ALL_STOCK_IDS}, USE_TEST_LOCAL_6_ITEMS {USE_TEST_LOCAL_6_ITEMS}')\n",
    "print(f'NBR_FOR_SUBSET_OF_STOCK_IDS: {NBR_FOR_SUBSET_OF_STOCK_IDS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a686d343-962d-45ab-90d1-cfeaf5a310c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility says ROOT is /home/ian/data/kaggle/optiver_volatility/\n",
      "In [2] used 0.0859 MiB RAM in 0.11s, peaked 0.00 MiB above current, total RAM usage 159.63 MiB\n"
     ]
    }
   ],
   "source": [
    "# OR PASTE IN UTILITY CODE HERE FOR KAGGLE\n",
    "from utility import make_unique_time_ids, get_training_stock_ids, rmspe_score\n",
    "from utility import ROOT, TEST_CSV, TRAIN_CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bebae2-9f9f-44b4-ac8a-6681b4866674",
   "metadata": {},
   "source": [
    "## Load train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333d41e0-407c-4ffe-ab6e-f6a3e0550e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428932, 1)\n",
      "In [3] used 20.9805 MiB RAM in 0.29s, peaked 7.46 MiB above current, total RAM usage 180.61 MiB\n"
     ]
    }
   ],
   "source": [
    "df_train_all = pd.read_csv(TRAIN_CSV)\n",
    "df_train_all = df_train_all.set_index(['stock_id', 'time_id'])\n",
    "print(df_train_all.shape)\n",
    "#rows_for_stock_id_0 = df_train_all.query('stock_id == 0').shape[0]\n",
    "#rows_for_stock_id_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "182dbc09-4924-4fd0-b3cc-22ed222267b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [5] used 175.0078 MiB RAM in 0.31s, peaked 0.00 MiB above current, total RAM usage 356.34 MiB\n"
     ]
    }
   ],
   "source": [
    "def load_book(ROOT, filename, stock_id):\n",
    "    df_book_train_stock_X = pd.read_parquet(os.path.join(ROOT, f\"{filename}/stock_id={stock_id}\"))\n",
    "    df_book_train_stock_X[\"stock_id\"] = stock_id\n",
    "    df_book_train_stock_X = df_book_train_stock_X.set_index(['stock_id', 'time_id'])\n",
    "    return df_book_train_stock_X\n",
    "\n",
    "df_book_train_stock_X = load_book(ROOT, 'book_train.parquet', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b852854c-236e-453e-9ae6-d598cf6d5652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2c] 428,932x1, 0 nulls, is_view True, is_single_block True, is_consolidated True\n",
      "In [6] used -14.0000 MiB RAM in 0.11s, peaked 0.00 MiB above current, total RAM usage 342.34 MiB\n"
     ]
    }
   ],
   "source": [
    "def show_details(df):\n",
    "    try:\n",
    "        nbr_index_levels = len(df.index.levels)\n",
    "    except AttributeError:\n",
    "        nbr_index_levels = 1\n",
    "    nbr_nulls = df.isnull().sum().sum()\n",
    "    #nulls_msg = \"Has no nulls\"\n",
    "    #if nbr_nulls==0:\n",
    "    nulls_msg = f\"{nbr_nulls} nulls\"\n",
    "    is_view_msg = f'is_view {df_train_all._data.is_view}'\n",
    "    is_single_block_msg = f'is_single_block {df_train_all._data.is_single_block}'\n",
    "    is_consolidated_msg = f'is_consolidated {df_train_all._data.is_consolidated()}'    \n",
    "    print(f'[{nbr_index_levels}c] {df.shape[0]:,}x{df.shape[1]:,}, {nulls_msg}, {is_view_msg}, {is_single_block_msg}, {is_consolidated_msg}')\n",
    "\n",
    "show_details(df_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079f3c35-eda1-43c1-bcc5-6c9d60ff5ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ask_size1_nunique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ask_size1_nunique\n",
       "stock_id time_id                   \n",
       "0        5                       67\n",
       "         11                      26\n",
       "         16                      22\n",
       "         31                      30\n",
       "         62                      54"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [8] used 142.0273 MiB RAM in 0.47s, peaked 40.01 MiB above current, total RAM usage 399.69 MiB\n"
     ]
    }
   ],
   "source": [
    "# make feature columns\n",
    "def make_features_stats(df_book, agg_type, cols):\n",
    "    features_var1 = df_book.groupby(['stock_id', 'time_id'])[cols].agg(agg_type)\n",
    "    #print(type(features_var1))\n",
    "    if isinstance(features_var1, pd.Series):\n",
    "        # .size yields a series not a df\n",
    "        #features_var1.name = str(agg_type)\n",
    "        features_var1 = pd.DataFrame(features_var1, columns=[agg_type])\n",
    "        #pass\n",
    "    else:\n",
    "        features_var1_col_names = [f\"{col}_{agg_type}\" for col in cols]\n",
    "        features_var1.columns = features_var1_col_names\n",
    "    return features_var1\n",
    "\n",
    "if True: # lightweight tests\n",
    "    df_book_train_stock_XX = pd.read_parquet(os.path.join(ROOT, f\"book_train.parquet/stock_id=0\"))\n",
    "    df_book_train_stock_XX[\"stock_id\"] = 0\n",
    "    df_book_train_stock_XX = df_book_train_stock_XX.set_index(['stock_id', 'time_id'])\n",
    "    display(make_features_stats(df_book_train_stock_XX, 'nunique', ['ask_size1']).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bad335b-b301-4e14-97c9-9e800cef7342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 ms ± 6.08 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "In [9] used -102.0781 MiB RAM in 1.52s, peaked 70.17 MiB above current, total RAM usage 297.61 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit make_features_stats(df_book_train_stock_XX, 'nunique', ['ask_size1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "403eded6-5ce3-4d83-ba57-ceba9d778020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [10] used 0.0156 MiB RAM in 0.32s, peaked 44.01 MiB above current, total RAM usage 297.63 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.216714 s\n",
       "File: /tmp/ipykernel_66873/3623217067.py\n",
       "Function: make_features_stats at line 2\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     2                                           def make_features_stats(df_book, agg_type, cols):\n",
       "     3         1     216386.0 216386.0     99.8      features_var1 = df_book.groupby(['stock_id', 'time_id'])[cols].agg(agg_type)\n",
       "     4                                               #print(type(features_var1))\n",
       "     5         1          5.0      5.0      0.0      if isinstance(features_var1, pd.Series):\n",
       "     6                                                   # .size yields a series not a df\n",
       "     7                                                   #features_var1.name = str(agg_type)\n",
       "     8                                                   features_var1 = pd.DataFrame(features_var1, columns=[agg_type])\n",
       "     9                                                   #pass\n",
       "    10                                               else:\n",
       "    11         1          5.0      5.0      0.0          features_var1_col_names = [f\"{col}_{agg_type}\" for col in cols]\n",
       "    12         1        317.0    317.0      0.1          features_var1.columns = features_var1_col_names\n",
       "    13         1          1.0      1.0      0.0      return features_var1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f make_features_stats make_features_stats(df_book_train_stock_XX, 'nunique', ['ask_size1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "faf1a31f-30c3-4ada-a2be-56d5b18f5331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.48036786359452\n",
      "In [73] used 0.2539 MiB RAM in 0.11s, peaked 0.00 MiB above current, total RAM usage 634.11 MiB\n"
     ]
    }
   ],
   "source": [
    "def _realized_volatility_weighted_sub(ser, weights):\n",
    "    ser_weighted = ser * weights\n",
    "    return np.sqrt(np.sum(ser_weighted**2))\n",
    "\n",
    "def realized_volatility_weighted(ser, weights_type):\n",
    "    \"\"\"Weighted volatility\"\"\"\n",
    "    # as a numpy array\n",
    "    # we drop from 12us to 3us by adding @njit to the _sub function\n",
    "    # we can't make _sub a closure, it loses all compilation benefits\n",
    "    # and we can't add njit(cache=True) in Jupyter as it can't\n",
    "    # find a cache location    \n",
    "    # as a Series we have 5us and 15us w/wo @njit respectively\n",
    "    if isinstance(ser, pd.Series):\n",
    "        ser = ser.to_numpy()\n",
    "    nbr_items = ser.shape[0]\n",
    "    if weights_type == 'uniform':\n",
    "        weights = np.ones(nbr_items)\n",
    "    elif weights_type == 'linear':\n",
    "        weights = np.linspace(0.1, 1, nbr_items) # linear increasing weight\n",
    "    elif weights_type == 'half0half1':\n",
    "        half_way = int(ser.shape[0] / 2)\n",
    "        weights = np.concatenate((np.zeros(half_way), np.ones(ser.shape[0] - half_way))) # 0s then 1s weight\n",
    "    elif weights_type == 'geom':\n",
    "        weights = np.geomspace(0.01, 1, nbr_items) # geometric increase\n",
    "    #assert isinstance(weights_type, str) == False, f\"Must not be a string like '{weights}' at this point\"\n",
    "    return _realized_volatility_weighted_sub(ser, weights)\n",
    "\n",
    "#from joblib import Memory\n",
    "#memory = Memory(\"joblib_cache\", verbose=0)\n",
    "#@memory.cache\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache\n",
    "def get_weights(nbr_items, weights_type):\n",
    "    if weights_type == 'uniform':\n",
    "        weights = np.ones(nbr_items)\n",
    "    elif weights_type == 'linear':\n",
    "        weights = np.linspace(0.1, 1, nbr_items) # linear increasing weight\n",
    "    elif weights_type == 'half0half1':\n",
    "        half_way = int(ser.shape[0] / 2)\n",
    "        weights = np.concatenate((np.zeros(half_way), np.ones(ser.shape[0] - half_way))) # 0s then 1s weight\n",
    "    elif weights_type == 'geom':\n",
    "        weights = np.geomspace(0.01, 1, nbr_items) # geometric increase\n",
    "    return weights\n",
    "\n",
    "def realized_volatility_weighted_v2(ser, weights_type):\n",
    "    \"\"\"Weighted volatility\"\"\"\n",
    "    # as a numpy array\n",
    "    # we drop from 12us to 3us by adding @njit to the _sub function\n",
    "    # we can't make _sub a closure, it loses all compilation benefits\n",
    "    # and we can't add njit(cache=True) in Jupyter as it can't\n",
    "    # find a cache location    \n",
    "    # as a Series we have 5us and 15us w/wo @njit respectively\n",
    "    if isinstance(ser, pd.Series):\n",
    "        ser = ser.to_numpy()\n",
    "    nbr_items = ser.shape[0]\n",
    "    weights = get_weights(nbr_items, weights_type)\n",
    "    #assert isinstance(weights_type, str) == False, f\"Must not be a string like '{weights}' at this point\"\n",
    "    return _realized_volatility_weighted_sub(ser, weights)\n",
    "\n",
    "series_log_return = pd.Series(np.linspace(0, 10, 600))\n",
    "print(realized_volatility_weighted(series_log_return, weights_type=\"uniform\"))\n",
    "assert realized_volatility_weighted(series_log_return, weights_type=\"uniform\") == realized_volatility_weighted_v2(series_log_return, weights_type=\"uniform\")\n",
    "    #%timeit realized_volatility_weighted(series_log_return, weights_type=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53f1eb07-ae0e-48c1-959d-b370a5f80868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5 µs ± 589 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "In [68] used 0.0000 MiB RAM in 1.66s, peaked 0.00 MiB above current, total RAM usage 633.83 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit realized_volatility_weighted(series_log_return, weights_type=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa5534cd-9e24-4751-a3d7-ad27108cd704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.9 µs ± 207 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "In [75] used 0.0000 MiB RAM in 11.42s, peaked 0.00 MiB above current, total RAM usage 634.11 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit realized_volatility_weighted_v2(series_log_return, weights_type=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "816c25e5-9a07-45c3-a47b-05893b3db940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [13] used 0.0312 MiB RAM in 0.11s, peaked 0.00 MiB above current, total RAM usage 297.68 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 7.9e-05 s\n",
       "File: /tmp/ipykernel_66873/3070439160.py\n",
       "Function: _realized_volatility_weighted_sub at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def _realized_volatility_weighted_sub(ser, weights):\n",
       "     2         1         24.0     24.0     30.4      ser_weighted = ser * weights\n",
       "     3         1         55.0     55.0     69.6      return np.sqrt(np.sum(ser_weighted**2))\n",
       "\n",
       "Total time: 0.00017 s\n",
       "File: /tmp/ipykernel_66873/3070439160.py\n",
       "Function: realized_volatility_weighted at line 5\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     5                                           def realized_volatility_weighted(ser, weights_type):\n",
       "     6                                               \"\"\"Weighted volatility\"\"\"\n",
       "     7                                               # as a numpy array\n",
       "     8                                               # we drop from 12us to 3us by adding @njit to the _sub function\n",
       "     9                                               # we can't make _sub a closure, it loses all compilation benefits\n",
       "    10                                               # and we can't add njit(cache=True) in Jupyter as it can't\n",
       "    11                                               # find a cache location    \n",
       "    12                                               # as a Series we have 5us and 15us w/wo @njit respectively\n",
       "    13         1          4.0      4.0      2.4      if isinstance(ser, pd.Series):\n",
       "    14         1         43.0     43.0     25.3          ser = ser.to_numpy()\n",
       "    15         1          3.0      3.0      1.8      nbr_items = ser.shape[0]\n",
       "    16         1          1.0      1.0      0.6      if weights_type == 'uniform':\n",
       "    17         1         37.0     37.0     21.8          weights = np.ones(nbr_items)\n",
       "    18                                               elif weights_type == 'linear':\n",
       "    19                                                   weights = np.linspace(0.1, 1, nbr_items) # linear increasing weight\n",
       "    20                                               elif weights_type == 'half0half1':\n",
       "    21                                                   half_way = int(ser.shape[0] / 2)\n",
       "    22                                                   weights = np.concatenate((np.zeros(half_way), np.ones(ser.shape[0] - half_way))) # 0s then 1s weight\n",
       "    23                                               elif weights_type == 'geom':\n",
       "    24                                                   weights = np.geomspace(0.01, 1, nbr_items) # geometric increase\n",
       "    25                                               #assert isinstance(weights_type, str) == False, f\"Must not be a string like '{weights}' at this point\"\n",
       "    26         1         82.0     82.0     48.2      return _realized_volatility_weighted_sub(ser, weights)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f realized_volatility_weighted -f _realized_volatility_weighted_sub realized_volatility_weighted(series_log_return, weights_type=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d530b07-9a93-4b78-bcb6-7cf4a4fbcbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>log_return2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  log_return2\n",
       "stock_id time_id             \n",
       "0        5           0.004500\n",
       "         11          0.001749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [14] used 89.9141 MiB RAM in 2.79s, peaked 39.50 MiB above current, total RAM usage 387.59 MiB\n"
     ]
    }
   ],
   "source": [
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "#def realized_volatility(series_log_return):\n",
    "#    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "def make_wap(df_book_data, num=1, wap_colname=\"wap\"):\n",
    "    \"\"\"Modifies df_book_data\"\"\"\n",
    "    assert num==1 or num==2\n",
    "    wap_numerator = (df_book_data[f'bid_price{num}'] * df_book_data[f'ask_size{num}'] +\n",
    "                                     df_book_data[f'ask_price{num}'] * df_book_data[f'bid_size{num}'])\n",
    "    wap_denominator = df_book_data[f'bid_size{num}'] + df_book_data[f'ask_size{num}']\n",
    "    df_book_data[wap_colname] = wap_numerator / wap_denominator\n",
    "\n",
    "#@memory.cache\n",
    "def make_realized_volatility(df_book_data, log_return_name='log_return', wap_colname='wap', weights=None):\n",
    "    \"\"\"Consume wap column\"\"\"\n",
    "    df_book_data[log_return_name] = df_book_data.groupby(['stock_id', 'time_id'])[wap_colname].apply(log_return)\n",
    "    df_book_data = df_book_data[~df_book_data[log_return_name].isnull()]\n",
    "    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['stock_id', 'time_id'])[log_return_name].agg(realized_volatility_weighted, weights))\n",
    "    return df_realized_vol_per_stock\n",
    "\n",
    "make_wap(df_book_train_stock_X, 2) # adds 'wap' column\n",
    "df_realized_vol_per_stockX = make_realized_volatility(df_book_train_stock_X, log_return_name=\"log_return2\", weights='linear')\n",
    "display(df_realized_vol_per_stockX.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eab6adc4-9b47-482e-af1c-43206a997db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ask1_bid1_diff_log_return_linear</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>2.032456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.646661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ask1_bid1_diff_log_return_linear\n",
       "stock_id time_id                                  \n",
       "0        5                                2.032456\n",
       "         11                               1.646661"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [15] used 9.5977 MiB RAM in 2.50s, peaked 55.87 MiB above current, total RAM usage 397.19 MiB\n"
     ]
    }
   ],
   "source": [
    "def make_volatility_ask_bid_diff(df_book_data, col='ask1_bid1_diff', weights='uniform'):\n",
    "    new_name = col + \"_log_return_\" + weights\n",
    "    df_book_data[new_name] = df_book_data.groupby(['stock_id', 'time_id'])[col].apply(log_return)\n",
    "    df_book_data = df_book_data[~df_book_data[new_name].isnull()]\n",
    "    # makes a new dataframe\n",
    "    df_realized_vol_ask_bid_diff_per_stock =  pd.DataFrame(df_book_data.groupby(['stock_id', 'time_id'])[new_name].agg(realized_volatility_weighted, weights))\n",
    "    return df_realized_vol_ask_bid_diff_per_stock\n",
    "\n",
    "df_book_train_stock_X['ask1_bid1_diff'] = (df_book_train_stock_X['ask_price1'] / df_book_train_stock_X['bid_price1']) - 1\n",
    "df_realized_vol_ask_bid_diff_per_stock = make_volatility_ask_bid_diff(df_book_train_stock_X, weights='linear')\n",
    "display(df_realized_vol_ask_bid_diff_per_stock.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d39146a5-755d-40ca-84d0-5390f8e71087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [18] used 0.0000 MiB RAM in 0.10s, peaked 0.00 MiB above current, total RAM usage 414.73 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%lprof` not found.\n"
     ]
    }
   ],
   "source": [
    "%lprof -f make_volatility_ask_bid_diff make_volatility_ask_bid_diff(df_book_train_stock_X, weights='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c82d71b-da98-41dc-b5be-ba53b12ff05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>log_return2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  log_return2\n",
       "stock_id time_id             \n",
       "0        5           0.004500\n",
       "         11          0.001749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [16] used -15.9570 MiB RAM in 2.48s, peaked 71.86 MiB above current, total RAM usage 381.23 MiB\n"
     ]
    }
   ],
   "source": [
    "def make_wap(df_book_data, num=1, wap_colname=\"wap\"):\n",
    "    \"\"\"Modifies df_book_data\"\"\"\n",
    "    assert num==1 or num==2\n",
    "    wap_numerator = (df_book_data[f'bid_price{num}'] * df_book_data[f'ask_size{num}'] +\n",
    "                                     df_book_data[f'ask_price{num}'] * df_book_data[f'bid_size{num}'])\n",
    "    wap_denominator = df_book_data[f'bid_size{num}'] + df_book_data[f'ask_size{num}']\n",
    "    df_book_data[wap_colname] = wap_numerator / wap_denominator\n",
    "\n",
    "def make_realized_volatility(df_book_data, log_return_name='log_return', wap_colname='wap', weights=None):\n",
    "    \"\"\"Consume wap column\"\"\"\n",
    "    df_book_data[log_return_name] = df_book_data.groupby(['stock_id', 'time_id'])[wap_colname].apply(log_return)\n",
    "    df_book_data = df_book_data[~df_book_data[log_return_name].isnull()]\n",
    "    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['stock_id', 'time_id'])[log_return_name].agg(realized_volatility_weighted, weights))\n",
    "    return df_realized_vol_per_stock\n",
    "\n",
    "make_wap(df_book_train_stock_X, 2) # adds 'wap' column\n",
    "df_realized_vol_per_stockX = make_realized_volatility(df_book_train_stock_X, log_return_name=\"log_return2\", weights='linear')\n",
    "display(df_realized_vol_per_stockX.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810cbe93-90c1-483a-a52d-abad0141ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [22] used -0.3008 MiB RAM in 5.23s, peaked 35.43 MiB above current, total RAM usage 428.05 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.090863 s\n",
       "File: /tmp/ipykernel_66873/3070439160.py\n",
       "Function: _realized_volatility_weighted_sub at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def _realized_volatility_weighted_sub(ser, weights):\n",
       "     2      3830       9869.0      2.6     10.9      ser_weighted = ser * weights\n",
       "     3      3830      80994.0     21.1     89.1      return np.sqrt(np.sum(ser_weighted**2))\n",
       "\n",
       "Total time: 0.456658 s\n",
       "File: /tmp/ipykernel_66873/3070439160.py\n",
       "Function: realized_volatility_weighted at line 5\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     5                                           def realized_volatility_weighted(ser, weights_type):\n",
       "     6                                               \"\"\"Weighted volatility\"\"\"\n",
       "     7                                               # as a numpy array\n",
       "     8                                               # we drop from 12us to 3us by adding @njit to the _sub function\n",
       "     9                                               # we can't make _sub a closure, it loses all compilation benefits\n",
       "    10                                               # and we can't add njit(cache=True) in Jupyter as it can't\n",
       "    11                                               # find a cache location    \n",
       "    12                                               # as a Series we have 5us and 15us w/wo @njit respectively\n",
       "    13      3830       5649.0      1.5      1.2      if isinstance(ser, pd.Series):\n",
       "    14      3830      46051.0     12.0     10.1          ser = ser.to_numpy()\n",
       "    15      3830       4128.0      1.1      0.9      nbr_items = ser.shape[0]\n",
       "    16      3830       2758.0      0.7      0.6      if weights_type == 'uniform':\n",
       "    17                                                   weights = np.ones(nbr_items)\n",
       "    18      3830       2340.0      0.6      0.5      elif weights_type == 'linear':\n",
       "    19      3830     293011.0     76.5     64.2          weights = np.linspace(0.1, 1, nbr_items) # linear increasing weight\n",
       "    20                                               elif weights_type == 'half0half1':\n",
       "    21                                                   half_way = int(ser.shape[0] / 2)\n",
       "    22                                                   weights = np.concatenate((np.zeros(half_way), np.ones(ser.shape[0] - half_way))) # 0s then 1s weight\n",
       "    23                                               elif weights_type == 'geom':\n",
       "    24                                                   weights = np.geomspace(0.01, 1, nbr_items) # geometric increase\n",
       "    25                                               #assert isinstance(weights_type, str) == False, f\"Must not be a string like '{weights}' at this point\"\n",
       "    26      3830     102721.0     26.8     22.5      return _realized_volatility_weighted_sub(ser, weights)\n",
       "\n",
       "Total time: 5.12457 s\n",
       "File: /tmp/ipykernel_66873/3434763625.py\n",
       "Function: make_realized_volatility at line 9\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     9                                           def make_realized_volatility(df_book_data, log_return_name='log_return', wap_colname='wap', weights=None):\n",
       "    10                                               \"\"\"Consume wap column\"\"\"\n",
       "    11         1    4137143.0 4137143.0     80.7      df_book_data[log_return_name] = df_book_data.groupby(['stock_id', 'time_id'])[wap_colname].apply(log_return)\n",
       "    12         1      29960.0  29960.0      0.6      df_book_data = df_book_data[~df_book_data[log_return_name].isnull()]\n",
       "    13         1     957468.0 957468.0     18.7      df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['stock_id', 'time_id'])[log_return_name].agg(realized_volatility_weighted, weights))\n",
       "    14         1          1.0      1.0      0.0      return df_realized_vol_per_stock\n",
       "\n",
       "Total time: 2.21686 s\n",
       "File: /tmp/ipykernel_66873/798300968.py\n",
       "Function: log_return at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def log_return(list_stock_prices):\n",
       "     2      3830    2216856.0    578.8    100.0      return np.log(list_stock_prices).diff()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f make_realized_volatility -f realized_volatility_weighted -f _realized_volatility_weighted_sub -f log_return make_realized_volatility(df_book_train_stock_X, log_return_name=\"log_return2\", weights='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c417e168-bbec-4402-9fef-5b90ea65a9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id  time_id\n",
       "0         5               NaN\n",
       "          5          0.000000\n",
       "          5          0.000001\n",
       "          5          0.000000\n",
       "          5          0.000000\n",
       "                       ...   \n",
       "          5          0.000652\n",
       "          5         -0.000631\n",
       "          5          0.000631\n",
       "          5         -0.000631\n",
       "          5          0.000631\n",
       "Name: wap, Length: 302, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [26] used 0.0000 MiB RAM in 0.13s, peaked 0.00 MiB above current, total RAM usage 428.05 MiB\n"
     ]
    }
   ],
   "source": [
    "ser = df_book_train_stock_X.query('stock_id ==0 and time_id == 5')['wap']\n",
    "log_return(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ca1dc72-81a9-43df-8a0f-025a93b99233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>wap</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>0.002517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.001665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       wap\n",
       "stock_id time_id          \n",
       "0        5        0.002517\n",
       "         11       0.000904\n",
       "         16       0.001504\n",
       "         31       0.001665\n",
       "         62       0.001402"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [58] used 1.0078 MiB RAM in 4.53s, peaked 1.62 MiB above current, total RAM usage 633.25 MiB\n"
     ]
    }
   ],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit(parallel=True)\n",
    "def log_return_v2_inner(arr):\n",
    "    arr = np.log(arr)\n",
    "    arr2 = np.concatenate((np.array([np.NaN]), arr[1:] - arr[:-1]))\n",
    "    return arr2\n",
    "\n",
    "def log_return_v2(list_stock_prices_ser):\n",
    "    # assume list_stock_prices is an array, not Series\n",
    "    #print(type(list_stock_prices))\n",
    "    list_stock_prices = list_stock_prices_ser.to_numpy()\n",
    "    arr2 = log_return_v2_inner(list_stock_prices)\n",
    "    return pd.Series(arr2, index=list_stock_prices_ser.index)\n",
    "\n",
    "\n",
    "def make_realized_volatility_v2(df_book_data, log_return_name='log_return', wap_colname='wap', weights=None):\n",
    "    \"\"\"Consume wap column\"\"\"\n",
    "    logged_diff = df_book_data.groupby(['stock_id', 'time_id'])[wap_colname].apply(log_return_v2)\n",
    "    #print(logged_diff[:5])\n",
    "    #df_book_data = df_book_data[~df_book_data[log_return_name].isnull()]\n",
    "    logged_diff = logged_diff.dropna()\n",
    "    df_realized_vol_per_stock =  pd.DataFrame(logged_diff.groupby(['stock_id', 'time_id']).agg(realized_volatility_weighted, weights))\n",
    "    return df_realized_vol_per_stock\n",
    "\n",
    "make_wap(df_book_train_stock_X, 1) # adds 'wap' column\n",
    "\n",
    "df_realized_vol_per_stockX_orig = make_realized_volatility_v2(df_book_train_stock_X, log_return_name=\"log_return2\", weights='linear')\n",
    "df_realized_vol_per_stockX = make_realized_volatility_v2(df_book_train_stock_X, log_return_name=\"log_return2\", weights='linear')\n",
    "display(df_realized_vol_per_stockX[:5])\n",
    "pd.testing.assert_frame_equal(df_realized_vol_per_stockX_orig, df_realized_vol_per_stockX)\n",
    "#log_return_v2(ser.to_numpy()) # this looks like the original, sans the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "50613d87-1d9b-4c7b-997e-7dbba3d1aa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "================================================================================\n",
      " Parallel Accelerator Optimizing:  Function log_return_v2_inner, \n",
      "/tmp/ipykernel_66873/1514267201.py (3)  \n",
      "================================================================================\n",
      "\n",
      "\n",
      "Parallel loop listing for  Function log_return_v2_inner, /tmp/ipykernel_66873/1514267201.py (3) \n",
      "-----------------------------------------------------------------------|loop #ID\n",
      "@njit(parallel=True)                                                   | \n",
      "def log_return_v2_inner(arr):                                          | \n",
      "    arr = np.log(arr)--------------------------------------------------| #0\n",
      "    arr2 = np.concatenate((np.array([np.NaN]), arr[1:] - arr[:-1]))----| #1\n",
      "    return arr2                                                        | \n",
      "------------------------------ After Optimisation ------------------------------\n",
      "Parallel structure is already optimal.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      " \n",
      "In [76] used 0.0000 MiB RAM in 0.11s, peaked 0.00 MiB above current, total RAM usage 634.11 MiB\n"
     ]
    }
   ],
   "source": [
    "log_return_v2_inner.parallel_diagnostics(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05b3ec42-48a5-4337-9ee4-2990ed802c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.63 s ± 46.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "In [77] used -0.0117 MiB RAM in 13.15s, peaked 1.32 MiB above current, total RAM usage 634.09 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit make_realized_volatility_v2(df_book_train_stock_X, log_return_name=\"log_return2\", weights='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b68b8936-dab7-4dfd-a029-42584d555260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11 s ± 17.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "In [57] used 16.2188 MiB RAM in 16.99s, peaked 2.71 MiB above current, total RAM usage 632.24 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit make_realized_volatility(df_book_train_stock_X, log_return_name=\"log_return2\", weights='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "232cc172-1bc3-4123-91ac-4e62be420cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [63] used 0.5625 MiB RAM in 4.13s, peaked 0.54 MiB above current, total RAM usage 633.83 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 1.08014 s\n",
       "File: /tmp/ipykernel_66873/1514267201.py\n",
       "Function: log_return_v2 at line 9\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     9                                           def log_return_v2(list_stock_prices_ser):\n",
       "    10                                               # assume list_stock_prices is an array, not Series\n",
       "    11                                               #print(type(list_stock_prices))\n",
       "    12      3830      58373.0     15.2      5.4      list_stock_prices = list_stock_prices_ser.to_numpy()\n",
       "    13      3830     403553.0    105.4     37.4      arr2 = log_return_v2_inner(list_stock_prices)\n",
       "    14      3830     618214.0    161.4     57.2      return pd.Series(arr2, index=list_stock_prices_ser.index)\n",
       "\n",
       "Total time: 4.02051 s\n",
       "File: /tmp/ipykernel_66873/1514267201.py\n",
       "Function: make_realized_volatility_v2 at line 17\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    17                                           def make_realized_volatility_v2(df_book_data, log_return_name='log_return', wap_colname='wap', weights=None):\n",
       "    18                                               \"\"\"Consume wap column\"\"\"\n",
       "    19         1    3060974.0 3060974.0     76.1      logged_diff = df_book_data.groupby(['stock_id', 'time_id'])[wap_colname].apply(log_return_v2)\n",
       "    20                                               #print(logged_diff[:5])\n",
       "    21                                               #df_book_data = df_book_data[~df_book_data[log_return_name].isnull()]\n",
       "    22         1       4977.0   4977.0      0.1      logged_diff = logged_diff.dropna()\n",
       "    23         1     954559.0 954559.0     23.7      df_realized_vol_per_stock =  pd.DataFrame(logged_diff.groupby(['stock_id', 'time_id']).agg(realized_volatility_weighted, weights))\n",
       "    24         1          1.0      1.0      0.0      return df_realized_vol_per_stock\n",
       "\n",
       "Total time: 0.092087 s\n",
       "File: /tmp/ipykernel_66873/3070439160.py\n",
       "Function: _realized_volatility_weighted_sub at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def _realized_volatility_weighted_sub(ser, weights):\n",
       "     2      3830       9848.0      2.6     10.7      ser_weighted = ser * weights\n",
       "     3      3830      82239.0     21.5     89.3      return np.sqrt(np.sum(ser_weighted**2))\n",
       "\n",
       "Total time: 0.463085 s\n",
       "File: /tmp/ipykernel_66873/3070439160.py\n",
       "Function: realized_volatility_weighted at line 5\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     5                                           def realized_volatility_weighted(ser, weights_type):\n",
       "     6                                               \"\"\"Weighted volatility\"\"\"\n",
       "     7                                               # as a numpy array\n",
       "     8                                               # we drop from 12us to 3us by adding @njit to the _sub function\n",
       "     9                                               # we can't make _sub a closure, it loses all compilation benefits\n",
       "    10                                               # and we can't add njit(cache=True) in Jupyter as it can't\n",
       "    11                                               # find a cache location    \n",
       "    12                                               # as a Series we have 5us and 15us w/wo @njit respectively\n",
       "    13      3830       5789.0      1.5      1.3      if isinstance(ser, pd.Series):\n",
       "    14      3830      47259.0     12.3     10.2          ser = ser.to_numpy()\n",
       "    15      3830       4174.0      1.1      0.9      nbr_items = ser.shape[0]\n",
       "    16      3830       2867.0      0.7      0.6      if weights_type == 'uniform':\n",
       "    17                                                   weights = np.ones(nbr_items)\n",
       "    18      3830       2403.0      0.6      0.5      elif weights_type == 'linear':\n",
       "    19      3830     296402.0     77.4     64.0          weights = np.linspace(0.1, 1, nbr_items) # linear increasing weight\n",
       "    20                                               elif weights_type == 'half0half1':\n",
       "    21                                                   half_way = int(ser.shape[0] / 2)\n",
       "    22                                                   weights = np.concatenate((np.zeros(half_way), np.ones(ser.shape[0] - half_way))) # 0s then 1s weight\n",
       "    23                                               elif weights_type == 'geom':\n",
       "    24                                                   weights = np.geomspace(0.01, 1, nbr_items) # geometric increase\n",
       "    25                                               #assert isinstance(weights_type, str) == False, f\"Must not be a string like '{weights}' at this point\"\n",
       "    26      3830     104191.0     27.2     22.5      return _realized_volatility_weighted_sub(ser, weights)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f make_realized_volatility_v2 -f realized_volatility_weighted -f _realized_volatility_weighted_sub -f log_return_v2 make_realized_volatility_v2(df_book_train_stock_X, log_return_name=\"log_return2\", weights='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48004be8-218d-4429-aa0b-b0501de94a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>bid_price1_var</th>\n",
       "      <th>ask_price1_var</th>\n",
       "      <th>bid_price2_var</th>\n",
       "      <th>ask_price2_var</th>\n",
       "      <th>bid_size1_var</th>\n",
       "      <th>ask_size1_var</th>\n",
       "      <th>bid_size2_var</th>\n",
       "      <th>ask_size2_var</th>\n",
       "      <th>bid_price1_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>log_return1_linear</th>\n",
       "      <th>log_return2_linear</th>\n",
       "      <th>log_return1_half0half1</th>\n",
       "      <th>log_return2_half0half1</th>\n",
       "      <th>ask1_bid1_diff_log_return_uniform</th>\n",
       "      <th>ask2_bid2_diff_log_return_half0half1</th>\n",
       "      <th>ask1_bid2_diff_log_return_uniform</th>\n",
       "      <th>ask2_bid1_diff_log_return_uniform</th>\n",
       "      <th>trade_size_count</th>\n",
       "      <th>trade_order_count_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">31</th>\n",
       "      <th>5</th>\n",
       "      <td>0.004113</td>\n",
       "      <td>2.692188e-06</td>\n",
       "      <td>2.681080e-06</td>\n",
       "      <td>2.692195e-06</td>\n",
       "      <td>2.681076e-06</td>\n",
       "      <td>2.238556e+07</td>\n",
       "      <td>3.623230e+07</td>\n",
       "      <td>1.806015e+07</td>\n",
       "      <td>1.258087e+07</td>\n",
       "      <td>0.997129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.981440</td>\n",
       "      <td>0.408059</td>\n",
       "      <td>0.574537</td>\n",
       "      <td>0.574467</td>\n",
       "      <td>50</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000956</td>\n",
       "      <td>2.032095e-08</td>\n",
       "      <td>2.031816e-08</td>\n",
       "      <td>2.032095e-08</td>\n",
       "      <td>2.031816e-08</td>\n",
       "      <td>4.570735e+07</td>\n",
       "      <td>1.831557e+08</td>\n",
       "      <td>1.766419e+07</td>\n",
       "      <td>1.628047e+08</td>\n",
       "      <td>1.000411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002127</td>\n",
       "      <td>1.158020e-06</td>\n",
       "      <td>1.158066e-06</td>\n",
       "      <td>1.157965e-06</td>\n",
       "      <td>1.157958e-06</td>\n",
       "      <td>1.800935e+08</td>\n",
       "      <td>2.033910e+08</td>\n",
       "      <td>6.437634e+08</td>\n",
       "      <td>2.730231e+08</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>12</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.003748</td>\n",
       "      <td>4.796240e-07</td>\n",
       "      <td>4.796240e-07</td>\n",
       "      <td>4.796240e-07</td>\n",
       "      <td>4.796240e-07</td>\n",
       "      <td>8.473959e+07</td>\n",
       "      <td>4.096441e+07</td>\n",
       "      <td>3.710387e+07</td>\n",
       "      <td>1.243901e+08</td>\n",
       "      <td>0.994919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>0.003457</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>30</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.001573</td>\n",
       "      <td>2.165609e-07</td>\n",
       "      <td>2.166966e-07</td>\n",
       "      <td>2.165884e-07</td>\n",
       "      <td>2.167498e-07</td>\n",
       "      <td>1.690191e+08</td>\n",
       "      <td>1.316160e+08</td>\n",
       "      <td>6.440620e+07</td>\n",
       "      <td>7.843908e+07</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.980168</td>\n",
       "      <td>0.406906</td>\n",
       "      <td>0.573358</td>\n",
       "      <td>0.573510</td>\n",
       "      <td>21</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    target  bid_price1_var  ask_price1_var  bid_price2_var  \\\n",
       "stock_id time_id                                                             \n",
       "31       5        0.004113    2.692188e-06    2.681080e-06    2.692195e-06   \n",
       "         11       0.000956    2.032095e-08    2.031816e-08    2.032095e-08   \n",
       "         16       0.002127    1.158020e-06    1.158066e-06    1.157965e-06   \n",
       "         31       0.003748    4.796240e-07    4.796240e-07    4.796240e-07   \n",
       "         62       0.001573    2.165609e-07    2.166966e-07    2.165884e-07   \n",
       "\n",
       "                  ask_price2_var  bid_size1_var  ask_size1_var  bid_size2_var  \\\n",
       "stock_id time_id                                                                \n",
       "31       5          2.681076e-06   2.238556e+07   3.623230e+07   1.806015e+07   \n",
       "         11         2.031816e-08   4.570735e+07   1.831557e+08   1.766419e+07   \n",
       "         16         1.157958e-06   1.800935e+08   2.033910e+08   6.437634e+08   \n",
       "         31         4.796240e-07   8.473959e+07   4.096441e+07   3.710387e+07   \n",
       "         62         2.167498e-07   1.690191e+08   1.316160e+08   6.440620e+07   \n",
       "\n",
       "                  ask_size2_var  bid_price1_mean  ...  log_return1_linear  \\\n",
       "stock_id time_id                                  ...                       \n",
       "31       5         1.258087e+07         0.997129  ...            0.002836   \n",
       "         11        1.628047e+08         1.000411  ...            0.000324   \n",
       "         16        2.730231e+08         0.999483  ...            0.001487   \n",
       "         31        1.243901e+08         0.994919  ...            0.001739   \n",
       "         62        7.843908e+07         0.999969  ...            0.001121   \n",
       "\n",
       "                  log_return2_linear  log_return1_half0half1  \\\n",
       "stock_id time_id                                               \n",
       "31       5                  0.003074                0.003343   \n",
       "         11                 0.001223                0.000292   \n",
       "         16                 0.002725                0.001602   \n",
       "         31                 0.004240                0.002036   \n",
       "         62                 0.001908                0.000994   \n",
       "\n",
       "                  log_return2_half0half1  ask1_bid1_diff_log_return_uniform  \\\n",
       "stock_id time_id                                                              \n",
       "31       5                      0.003636                           0.981440   \n",
       "         11                     0.000252                           0.001358   \n",
       "         16                     0.002811                           0.002366   \n",
       "         31                     0.005161                           0.003457   \n",
       "         62                     0.001528                           0.980168   \n",
       "\n",
       "                  ask2_bid2_diff_log_return_half0half1  \\\n",
       "stock_id time_id                                         \n",
       "31       5                                    0.408059   \n",
       "         11                                   0.000000   \n",
       "         16                                   0.001364   \n",
       "         31                                   0.002758   \n",
       "         62                                   0.406906   \n",
       "\n",
       "                  ask1_bid2_diff_log_return_uniform  \\\n",
       "stock_id time_id                                      \n",
       "31       5                                 0.574537   \n",
       "         11                                0.001260   \n",
       "         16                                0.002336   \n",
       "         31                                0.003121   \n",
       "         62                                0.573358   \n",
       "\n",
       "                  ask2_bid1_diff_log_return_uniform  trade_size_count  \\\n",
       "stock_id time_id                                                        \n",
       "31       5                                 0.574467                50   \n",
       "         11                                0.001261                10   \n",
       "         16                                0.002442                12   \n",
       "         31                                0.003269                30   \n",
       "         62                                0.573510                21   \n",
       "\n",
       "                  trade_order_count_sum  \n",
       "stock_id time_id                         \n",
       "31       5                          582  \n",
       "         11                          43  \n",
       "         16                          69  \n",
       "         31                         149  \n",
       "         62                         179  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [17] used 33.4922 MiB RAM in 118.91s, peaked 524.11 MiB above current, total RAM usage 414.73 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 77.6965 s\n",
       "File: /tmp/ipykernel_66873/831036116.py\n",
       "Function: load_data_build_features at line 2\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     2                                           def load_data_build_features(stock_id, ROOT, book_filename, trade_filename, cols, df_target):\n",
       "     3                                               # filename e.g. book_train.parquet\n",
       "     4         1         14.0     14.0      0.0      assert isinstance(stock_id, int)\n",
       "     5         2     244754.0 122377.0      0.3      df_book_stock_X = pd.read_parquet(\n",
       "     6         1         39.0     39.0      0.0          os.path.join(ROOT, f\"{book_filename}/stock_id={stock_id}\")\n",
       "     7                                               )\n",
       "     8         1       3274.0   3274.0      0.0      df_book_stock_X[\"stock_id\"] = stock_id\n",
       "     9         1     117988.0 117988.0      0.2      df_book_stock_X = df_book_stock_X.set_index(['stock_id', 'time_id'])\n",
       "    10                                               #assert df_book_train_stock_X.shape[0] > rows_for_stock_id_0, (df_book_train_stock_X.shape[0], rows_for_stock_id_0)\n",
       "    11                                               \n",
       "    12         2       9738.0   4869.0      0.0      df_trade_stock_X = pd.read_parquet(\n",
       "    13         1         17.0     17.0      0.0          os.path.join(ROOT, f\"{trade_filename}/stock_id={stock_id}\")\n",
       "    14                                               )\n",
       "    15         1        859.0    859.0      0.0      df_trade_stock_X[\"stock_id\"] = stock_id\n",
       "    16         1       7688.0   7688.0      0.0      df_trade_stock_X = df_trade_stock_X.set_index(['stock_id', 'time_id'])\n",
       "    17                                               \n",
       "    18                                               #df_book_train_stock_X_gt500 = df_book_train_stock_X.query(\"seconds_in_bucket>500\").copy()\n",
       "    19                                               #df_realized_vol_per_stock_short500 = add_wap_make_realized_volatility(df_book_train_stock_X_gt500, log_return_name='log_return_gt500sec')\n",
       "    20                                               #df_book_train_stock_X_gt300 = df_book_train_stock_X.query(\"seconds_in_bucket>300\").copy()\n",
       "    21                                               #df_realized_vol_per_stock_short300 = add_wap_make_realized_volatility(df_book_train_stock_X_gt300, log_return_name='log_return_gt300sec')\n",
       "    22                                               if True:\n",
       "    23         1      10610.0  10610.0      0.0          make_wap(df_book_stock_X, 2, \"wap2\") \n",
       "    24         1    5920997.0 5920997.0      7.6          df_realized_vol_per_stock_wap2_uniform = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return2_uniform\", wap_colname=\"wap2\", weights='uniform')    \n",
       "    25         1    4997886.0 4997886.0      6.4          df_realized_vol_per_stock_wap2_linear = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return2_linear\", wap_colname=\"wap2\", weights='linear')\n",
       "    26         1    4710368.0 4710368.0      6.1          df_realized_vol_per_stock_wap2_half0half1 = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return2_half0half1\", wap_colname=\"wap2\", weights='half0half1')\n",
       "    27         1      10096.0  10096.0      0.0          make_wap(df_book_stock_X, 1, \"wap\") # adds 'wap' column\n",
       "    28         1    5212721.0 5212721.0      6.7          df_realized_vol_per_stock_wap1_uniform = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return1_uniform\", weights='uniform')\n",
       "    29         1    5159133.0 5159133.0      6.6          df_realized_vol_per_stock_wap1_linear = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return1_linear\", weights='linear')\n",
       "    30         1    4763079.0 4763079.0      6.1          df_realized_vol_per_stock_wap1_half0half1 = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return1_half0half1\", weights='half0half1')\n",
       "    31                                           \n",
       "    32         1     200006.0 200006.0      0.3          features_var1 = make_features_stats(df_book_stock_X, 'var', cols)\n",
       "    33         1     182758.0 182758.0      0.2          features_mean1 = make_features_stats(df_book_stock_X, 'mean', cols)\n",
       "    34         1      94843.0  94843.0      0.1          features_size1 = make_features_stats(df_book_stock_X, 'size', cols)\n",
       "    35         1     169653.0 169653.0      0.2          features_min1 = make_features_stats(df_book_stock_X, 'min', cols)\n",
       "    36         1     163547.0 163547.0      0.2          features_max1 = make_features_stats(df_book_stock_X, 'max', cols)\n",
       "    37         1    1860092.0 1860092.0      2.4          features_nunique1 = make_features_stats(df_book_stock_X, 'nunique', cols)\n",
       "    38                                           \n",
       "    39         1       4951.0   4951.0      0.0          df_book_stock_X['ask1_bid1_diff'] = (df_book_stock_X['ask_price1'] / df_book_stock_X['bid_price1']) - 1\n",
       "    40         1    4846508.0 4846508.0      6.2          df_realized_vol_ask1_bid1_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask1_bid1_diff')\n",
       "    41         1    5537648.0 5537648.0      7.1          df_realized_vol_ask1_bid1_diff_per_stock_linear = make_volatility_ask_bid_diff(df_book_stock_X, col='ask1_bid1_diff', weights='linear')\n",
       "    42         1    5684934.0 5684934.0      7.3          df_realized_vol_ask1_bid1_diff_per_stock_linear = make_volatility_ask_bid_diff(df_book_stock_X, col='ask1_bid1_diff', weights='half0half1')\n",
       "    43         1       4889.0   4889.0      0.0          df_book_stock_X['ask2_bid2_diff'] = (df_book_stock_X['ask_price2'] / df_book_stock_X['bid_price2']) - 1\n",
       "    44         1    5661197.0 5661197.0      7.3          df_realized_vol_ask2_bid2_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask2_bid2_diff')\n",
       "    45         1    5624196.0 5624196.0      7.2          df_realized_vol_ask2_bid2_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask2_bid2_diff', weights='linear')\n",
       "    46         1    6293989.0 6293989.0      8.1          df_realized_vol_ask2_bid2_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask2_bid2_diff', weights='half0half1')\n",
       "    47         1       5283.0   5283.0      0.0          df_book_stock_X['ask1_bid2_diff'] = (df_book_stock_X['ask_price1'] / df_book_stock_X['bid_price2']) - 1\n",
       "    48         1    5311923.0 5311923.0      6.8          df_realized_vol_ask1_bid2_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask1_bid2_diff')\n",
       "    49         1       4964.0   4964.0      0.0          df_book_stock_X['ask2_bid1_diff'] = (df_book_stock_X['ask_price2'] / df_book_stock_X['bid_price1']) - 1\n",
       "    50         1    4786187.0 4786187.0      6.2          df_realized_vol_ask2_bid1_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask2_bid1_diff')\n",
       "    51                                               else:\n",
       "    52                                                   features_var1 = make_features_stats(df_book_stock_X, 'var', cols)\n",
       "    53                                                   \n",
       "    54                                               #breakpoint()\n",
       "    55                                               # trade stats\n",
       "    56         1      19531.0  19531.0      0.0      df_trade_basic_stats = df_trade_stock_X.groupby(['stock_id', 'time_id']).agg(trade_size_count=pd.NamedAgg('size', 'count'), trade_order_count_sum=pd.NamedAgg('order_count', 'sum'))\n",
       "    57                                           \n",
       "    58         1       7416.0   7416.0      0.0      df_train_stock_X = df_target.query('stock_id == @stock_id')\n",
       "    59                                               if True:\n",
       "    60         2          5.0      2.5      0.0          to_merge_book = [df_train_stock_X, \n",
       "    61         1          1.0      1.0      0.0                      features_var1, features_mean1, features_size1, \n",
       "    62         1          2.0      2.0      0.0                      features_min1, features_max1, features_nunique1,\n",
       "    63         1          2.0      2.0      0.0                      df_realized_vol_per_stock_wap1_uniform,\n",
       "    64         1          1.0      1.0      0.0                      df_realized_vol_ask1_bid1_diff_per_stock_linear,\n",
       "    65         1          2.0      2.0      0.0                      df_realized_vol_per_stock_wap2_uniform,\n",
       "    66         1          2.0      2.0      0.0                      df_realized_vol_per_stock_wap1_linear,\n",
       "    67         1          1.0      1.0      0.0                      df_realized_vol_per_stock_wap2_linear,\n",
       "    68         1          1.0      1.0      0.0                      df_realized_vol_per_stock_wap1_half0half1,\n",
       "    69         1          1.0      1.0      0.0                      df_realized_vol_per_stock_wap2_half0half1,\n",
       "    70         1          1.0      1.0      0.0                      df_realized_vol_ask1_bid1_diff_per_stock,\n",
       "    71         1          2.0      2.0      0.0                      df_realized_vol_ask2_bid2_diff_per_stock,\n",
       "    72         1          1.0      1.0      0.0                      df_realized_vol_ask1_bid2_diff_per_stock,\n",
       "    73         1          1.0      1.0      0.0                      df_realized_vol_ask2_bid1_diff_per_stock,]\n",
       "    74         1          1.0      1.0      0.0          to_merge_trade = [df_trade_basic_stats]\n",
       "    75                                               else:\n",
       "    76                                                   to_merge_book = [df_train_stock_X, features_var1]\n",
       "    77                                                   to_merge_trade = [df_trade_basic_stats]\n",
       "    78                                           \n",
       "    79                                               # some trade datasets are missing some time_ids, making the join a mess\n",
       "    80                                               # we reindex and make the choice to fillna 0\n",
       "    81         1      47127.0  47127.0      0.1      to_merge_trade = [to_merge_tr.reindex(to_merge_book[0].index, fill_value=0) for to_merge_tr in to_merge_trade]\n",
       "    82                                               #to_merge_trade = [to_merge_tr.reindex(to_merge_book[0].index, fill_value=0).fillna(0) for to_merge_tr in to_merge_trade]\n",
       "    83                                               \n",
       "    84         1         85.0     85.0      0.0      row_lengths = [df.shape[0] for df in to_merge_book]\n",
       "    85         1          4.0      4.0      0.0      assert len(set(row_lengths)) == 1, f\"row_lengths are different for stock {stock_id}: {row_lengths}\" # should all be same length\n",
       "    86         1          2.0      2.0      0.0      to_merge = to_merge_book + to_merge_trade\n",
       "    87        20         32.0      1.6      0.0      for idx, item_to_merge in enumerate(to_merge):\n",
       "    88        19         69.0      3.6      0.0          assert item_to_merge.index.names == ['stock_id', 'time_id'], f\"We must have the same index on idx {idx}\"\n",
       "    89         1      12718.0  12718.0      0.0      train_merged = pd.concat(to_merge, axis=1)\n",
       "    90                                                                        \n",
       "    91         1         29.0     29.0      0.0      if 'target' in train_merged.columns:\n",
       "    92         1       2624.0   2624.0      0.0          features = train_merged.drop(columns='target').columns\n",
       "    93                                                   #print(features)\n",
       "    94         1         21.0     21.0      0.0          assert len(set(features)) == len(features), f\"Feature duplication! {len(set(features))} vs {len(features)}\"\n",
       "    95                                           \n",
       "    96         1          2.0      2.0      0.0      return train_merged"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@memory.cache\n",
    "def load_data_build_features(stock_id, ROOT, book_filename, trade_filename, cols, df_target):\n",
    "    # filename e.g. book_train.parquet\n",
    "    assert isinstance(stock_id, int)\n",
    "    df_book_stock_X = pd.read_parquet(\n",
    "        os.path.join(ROOT, f\"{book_filename}/stock_id={stock_id}\")\n",
    "    )\n",
    "    df_book_stock_X[\"stock_id\"] = stock_id\n",
    "    df_book_stock_X = df_book_stock_X.set_index(['stock_id', 'time_id'])\n",
    "    #assert df_book_train_stock_X.shape[0] > rows_for_stock_id_0, (df_book_train_stock_X.shape[0], rows_for_stock_id_0)\n",
    "    \n",
    "    df_trade_stock_X = pd.read_parquet(\n",
    "        os.path.join(ROOT, f\"{trade_filename}/stock_id={stock_id}\")\n",
    "    )\n",
    "    df_trade_stock_X[\"stock_id\"] = stock_id\n",
    "    df_trade_stock_X = df_trade_stock_X.set_index(['stock_id', 'time_id'])\n",
    "    \n",
    "    #df_book_train_stock_X_gt500 = df_book_train_stock_X.query(\"seconds_in_bucket>500\").copy()\n",
    "    #df_realized_vol_per_stock_short500 = add_wap_make_realized_volatility(df_book_train_stock_X_gt500, log_return_name='log_return_gt500sec')\n",
    "    #df_book_train_stock_X_gt300 = df_book_train_stock_X.query(\"seconds_in_bucket>300\").copy()\n",
    "    #df_realized_vol_per_stock_short300 = add_wap_make_realized_volatility(df_book_train_stock_X_gt300, log_return_name='log_return_gt300sec')\n",
    "    if True:\n",
    "        make_wap(df_book_stock_X, 2, \"wap2\") \n",
    "        df_realized_vol_per_stock_wap2_uniform = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return2_uniform\", wap_colname=\"wap2\", weights='uniform')    \n",
    "        df_realized_vol_per_stock_wap2_linear = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return2_linear\", wap_colname=\"wap2\", weights='linear')\n",
    "        df_realized_vol_per_stock_wap2_half0half1 = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return2_half0half1\", wap_colname=\"wap2\", weights='half0half1')\n",
    "        make_wap(df_book_stock_X, 1, \"wap\") # adds 'wap' column\n",
    "        df_realized_vol_per_stock_wap1_uniform = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return1_uniform\", weights='uniform')\n",
    "        df_realized_vol_per_stock_wap1_linear = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return1_linear\", weights='linear')\n",
    "        df_realized_vol_per_stock_wap1_half0half1 = make_realized_volatility(df_book_stock_X, log_return_name=\"log_return1_half0half1\", weights='half0half1')\n",
    "\n",
    "        features_var1 = make_features_stats(df_book_stock_X, 'var', cols)\n",
    "        features_mean1 = make_features_stats(df_book_stock_X, 'mean', cols)\n",
    "        features_size1 = make_features_stats(df_book_stock_X, 'size', cols)\n",
    "        features_min1 = make_features_stats(df_book_stock_X, 'min', cols)\n",
    "        features_max1 = make_features_stats(df_book_stock_X, 'max', cols)\n",
    "        features_nunique1 = make_features_stats(df_book_stock_X, 'nunique', cols)\n",
    "\n",
    "        df_book_stock_X['ask1_bid1_diff'] = (df_book_stock_X['ask_price1'] / df_book_stock_X['bid_price1']) - 1\n",
    "        df_realized_vol_ask1_bid1_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask1_bid1_diff')\n",
    "        df_realized_vol_ask1_bid1_diff_per_stock_linear = make_volatility_ask_bid_diff(df_book_stock_X, col='ask1_bid1_diff', weights='linear')\n",
    "        df_realized_vol_ask1_bid1_diff_per_stock_linear = make_volatility_ask_bid_diff(df_book_stock_X, col='ask1_bid1_diff', weights='half0half1')\n",
    "        df_book_stock_X['ask2_bid2_diff'] = (df_book_stock_X['ask_price2'] / df_book_stock_X['bid_price2']) - 1\n",
    "        df_realized_vol_ask2_bid2_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask2_bid2_diff')\n",
    "        df_realized_vol_ask2_bid2_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask2_bid2_diff', weights='linear')\n",
    "        df_realized_vol_ask2_bid2_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask2_bid2_diff', weights='half0half1')\n",
    "        df_book_stock_X['ask1_bid2_diff'] = (df_book_stock_X['ask_price1'] / df_book_stock_X['bid_price2']) - 1\n",
    "        df_realized_vol_ask1_bid2_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask1_bid2_diff')\n",
    "        df_book_stock_X['ask2_bid1_diff'] = (df_book_stock_X['ask_price2'] / df_book_stock_X['bid_price1']) - 1\n",
    "        df_realized_vol_ask2_bid1_diff_per_stock = make_volatility_ask_bid_diff(df_book_stock_X, col='ask2_bid1_diff')\n",
    "    else:\n",
    "        features_var1 = make_features_stats(df_book_stock_X, 'var', cols)\n",
    "        \n",
    "    #breakpoint()\n",
    "    # trade stats\n",
    "    df_trade_basic_stats = df_trade_stock_X.groupby(['stock_id', 'time_id']).agg(trade_size_count=pd.NamedAgg('size', 'count'), trade_order_count_sum=pd.NamedAgg('order_count', 'sum'))\n",
    "\n",
    "    df_train_stock_X = df_target.query('stock_id == @stock_id')\n",
    "    if True:\n",
    "        to_merge_book = [df_train_stock_X, \n",
    "                    features_var1, features_mean1, features_size1, \n",
    "                    features_min1, features_max1, features_nunique1,\n",
    "                    df_realized_vol_per_stock_wap1_uniform,\n",
    "                    df_realized_vol_ask1_bid1_diff_per_stock_linear,\n",
    "                    df_realized_vol_per_stock_wap2_uniform,\n",
    "                    df_realized_vol_per_stock_wap1_linear,\n",
    "                    df_realized_vol_per_stock_wap2_linear,\n",
    "                    df_realized_vol_per_stock_wap1_half0half1,\n",
    "                    df_realized_vol_per_stock_wap2_half0half1,\n",
    "                    df_realized_vol_ask1_bid1_diff_per_stock,\n",
    "                    df_realized_vol_ask2_bid2_diff_per_stock,\n",
    "                    df_realized_vol_ask1_bid2_diff_per_stock,\n",
    "                    df_realized_vol_ask2_bid1_diff_per_stock,]\n",
    "        to_merge_trade = [df_trade_basic_stats]\n",
    "    else:\n",
    "        to_merge_book = [df_train_stock_X, features_var1]\n",
    "        to_merge_trade = [df_trade_basic_stats]\n",
    "\n",
    "    # some trade datasets are missing some time_ids, making the join a mess\n",
    "    # we reindex and make the choice to fillna 0\n",
    "    to_merge_trade = [to_merge_tr.reindex(to_merge_book[0].index, fill_value=0) for to_merge_tr in to_merge_trade]\n",
    "    #to_merge_trade = [to_merge_tr.reindex(to_merge_book[0].index, fill_value=0).fillna(0) for to_merge_tr in to_merge_trade]\n",
    "    \n",
    "    row_lengths = [df.shape[0] for df in to_merge_book]\n",
    "    assert len(set(row_lengths)) == 1, f\"row_lengths are different for stock {stock_id}: {row_lengths}\" # should all be same length\n",
    "    to_merge = to_merge_book + to_merge_trade\n",
    "    for idx, item_to_merge in enumerate(to_merge):\n",
    "        assert item_to_merge.index.names == ['stock_id', 'time_id'], f\"We must have the same index on idx {idx}\"\n",
    "    train_merged = pd.concat(to_merge, axis=1)\n",
    "                             \n",
    "    if 'target' in train_merged.columns:\n",
    "        features = train_merged.drop(columns='target').columns\n",
    "        #print(features)\n",
    "        assert len(set(features)) == len(features), f\"Feature duplication! {len(set(features))} vs {len(features)}\"\n",
    "\n",
    "    return train_merged\n",
    "\n",
    "#if 'memory' in dir():\n",
    "#    # only setup local cache if we're running locally in development\n",
    "#    load_data_build_features = memory.cache(load_data_build_features)\n",
    "    \n",
    "cols = ['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2',] \n",
    "cols += ['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2']\n",
    "\n",
    "# test...\n",
    "train_mergedXX = load_data_build_features(31, ROOT, 'book_train.parquet', 'trade_train.parquet', cols, df_train_all)\n",
    "display(train_mergedXX.head(5))\n",
    "\n",
    "%lprun -f load_data_build_features load_data_build_features(31, ROOT, 'book_train.parquet', 'trade_train.parquet', cols, df_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d6907-122e-41c2-b3cd-193c513038b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
